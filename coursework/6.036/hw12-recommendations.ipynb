{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #12 for Spring2020 6.036 (MIT OLL)\n",
    "\n",
    "Although I've done all the preceding hws, with HW12 I've switched to doing these assignments in Jupyter and uploading to GitHub so I can more easily review my work.\n",
    "\n",
    "Much of the code is supplied by the class, and much of the homework is filling in specific lines of code.  That is to say, I don't own most of this code, but it is publically available and usable under standard non-commercial licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin class-provided code and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "genres = ['Western', 'Comedy', 'Children', 'Crime', 'Musical',\n",
    "          'Adventure', 'Drama', 'Horror', 'War', 'Documentary',\n",
    "          'Romance', 'Animation', 'Film-Noir', 'Sci-Fi', 'Mystery',\n",
    "          'Fantasy', 'IMAX', 'Action', 'Thriller']\n",
    "\n",
    "# Data is a list of (a, i, r) triples\n",
    "ratings_small = \\\n",
    "[(0, 0, 5), (0, 1, 3), (0, 3, 1),\n",
    " (1, 0, 4), (1, 3, 1),\n",
    " (2, 0, 1), (2, 1, 1), (2, 3, 5),\n",
    " (3, 0, 1), (3, 3, 4),\n",
    " (4, 1, 1), (4, 2, 5), (4, 3, 4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(data, x):\n",
    "    (a, i, r) = data\n",
    "    (u, b_u, v, b_v) = x\n",
    "    return np.dot(u[a].T,v[i]) + b_u[a] + b_v[i]\n",
    "\n",
    "# X : n x k\n",
    "# Y : n\n",
    "def ridge_analytic(X, Y, lam):\n",
    "    (n, k) = X.shape\n",
    "    xm = np.mean(X, axis = 0, keepdims = True)   # 1 x n\n",
    "    ym = np.mean(Y)                              # 1 x 1\n",
    "    Z = X - xm                                   # d x n\n",
    "    T = Y - ym                                   # 1 x n\n",
    "    th = np.linalg.solve(np.dot(Z.T, Z) + lam * np.identity(k), np.dot(Z.T, T))\n",
    "    # th_0 account for the centering\n",
    "    th_0 = (ym - np.dot(xm, th))                 # 1 x 1\n",
    "    return th.reshape((k,1)), float(th_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Some movies are more equal than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With offsets [[ 0.22024566]\n",
      " [-0.22193986]] 0.007623888182955896\n",
      "With no offsets [[0.50148126]\n",
      " [0.0562376 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example from lab handout\n",
    "Z = np.array([[1], [1], [5], [1], [5], [5], [1]])\n",
    "b_v = np.array([[3], [3], [3], [3], [3], [5], [1]])\n",
    "B = np.array([[1, 10], [1, 10], [10, 1], [1, 10], [10, 1], [5, 5], [5, 5]])\n",
    "# Solution with offsets, using ridge_analytic provided in code file\n",
    "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
    "print('With offsets', u_a, b_u_a)\n",
    "# Solution using previous model, with no offsets\n",
    "u_a_no_b = np.dot(np.linalg.inv(np.dot(B.T, B) + 1 * np.identity(2)), np.dot(B.T, Z))\n",
    "print('With no offsets', u_a_no_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2A\n",
    "\n",
    "The most difficult part of this was remembering the b_v_i offset for \"bad ratings from everyone\".  I originally got 1.988 as my answer, which was rejected by the autograder before I remembered to add it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.98814062]] [[5.07105019]]\n"
     ]
    }
   ],
   "source": [
    "llama = np.array([[10],[1]])\n",
    "b_v_i = 1\n",
    "print(u_a.T@llama + b_u_a + b_v_i,u_a_no_b.T @ llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2B and 2C\n",
    "\n",
    "trivial now that I remember b_v_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.00847099]] [[1.0638573]]\n",
      "[[1.00847099]] [[1.0638573]]\n"
     ]
    }
   ],
   "source": [
    "robot = np.array([[1],[10]])\n",
    "b_v_i = 5\n",
    "print(u_a.T@robot + b_u_a + b_v_i,u_a_no_b.T @ robot)\n",
    "b_v_i = 3\n",
    "print(u_a.T@robot + b_u_a + b_v_i,u_a_no_b.T @ robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Implementing recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SGD outer loop\n",
    "def mf_sgd(data_train, data_validate, step_size_fn, k=2, lam=0.02, max_iter=100, verbose=False):\n",
    "    # size of the problem\n",
    "    ndata = len(data_train)\n",
    "    n = max(d[0] for d in data_train)+1\n",
    "    m = max(d[1] for d in data_train)+1\n",
    "    # Distribute the lambda among the users and items\n",
    "    lam_uv = lam/counts(data_train,0), lam/counts(data_train,1)\n",
    "    # Initial guess at u, b_u, v, b_v (also b)\n",
    "    x = ([np.random.normal(1/k, size=(k,1)) for j in range(n)],\n",
    "         np.zeros(n),\n",
    "         [np.random.normal(1/k, size=(k,1)) for j in range(m)],\n",
    "         np.zeros(m))\n",
    "    di = int(max_iter/10.)\n",
    "    for i in range(max_iter):\n",
    "        if i%di == 0 and verbose:\n",
    "            print('i=', i, 'train rmse=', rmse(data_train, x),\n",
    "                  'validate rmse', data_validate and rmse(data_validate, x))\n",
    "        step = step_size_fn(i)\n",
    "        j = np.random.randint(ndata)            # pick data item\n",
    "        sgd_step(data_train[j], x, lam_uv, step) # modify x\n",
    "    print('SGD result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) update_U\n",
    "\n",
    "the major issue here seems to be translating the phrasing of the regression from the lab.  Go slow and methodically and things make sense.\n",
    "Try to make impatient jumps of logic and nothing makes sense, so go step by step!\n",
    "\n",
    "Don't miss this:  \"Note that u and v are lists of column vectors (rows of U, V).\"   ie they're not arrays!\n",
    "\n",
    "In the lab, Ia is the set of all movies rated by person a.\n",
    "Ba is constructed by taking V and removing every movie not in Ia.\n",
    "Y is the (incomplete) matrix of all the ratings we have (dimension of n users x m movies)\n",
    "Then Za is a column vector of the YAi elements for all i in Ia\n",
    "Then call ridge analytic(B, (Z - b_v), lam)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_U_test()\n",
    "def update_U(data, vs_from_u, x, k, lam):\n",
    "    (u, b_u, v, b_v) = x\n",
    "    \n",
    "    # for each user\n",
    "    for i in range(len(vs_from_u)):\n",
    "        if not vs_from_u[i]: continue\n",
    "        c = len(vs_from_u[i])\n",
    "        #print(\"there are \", c, \" tuples in user \", i, \"'s records\")\n",
    "        indexes = []\n",
    "        B = np.array([[] for ix in range(k)])\n",
    "        score = np.zeros((c,1))\n",
    "        # for every rating they've done\n",
    "        for j in range(c):\n",
    "            \n",
    "            rating = vs_from_u[i][j]\n",
    "            #print(rating)\n",
    "            indexes.append(int(rating[0])) #construct Ia\n",
    "            score[j,0] = rating[1]\n",
    "            B = np.hstack((B,v[rating[0]]))\n",
    "        Z = score\n",
    "        B = B.T\n",
    "        b_v_reshape = b_v[indexes].reshape((b_v[indexes].shape[0],1)) # had to reshape this because the online checker uses a different format than the offline test code\n",
    "        #print(B.shape, Z.shape, b_v_reshape.shape)\n",
    "        th, th0 = ridge_analytic(B,Z-b_v_reshape,lam)\n",
    "        u[i] = th\n",
    "        b_u[i] = th0\n",
    "    x = (u,b_u,v,b_v)\n",
    "    return x\n",
    "\n",
    "\n",
    "## copied from homework solution since it's cleaner\n",
    "def update_V(data, us_from_v, x, k, lam):\n",
    "    (u, b_u, v, b_v) = x\n",
    "    for i in range(len(v)):\n",
    "        if not us_from_v[i]: continue\n",
    "        V = np.hstack([u[a] for (a, _) in us_from_v[i]]).T\n",
    "        y = np.array([r-b_u[a] for (a, r) in us_from_v[i]])\n",
    "        v[i], b_v[i] = ridge_analytic(V, y, lam)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Test case for update_U\n",
    "def update_U_test():\n",
    "  '''\n",
    "  This is a test function provided to help you debug your implementation\n",
    "  '''\n",
    "  k = 2\n",
    "  lam = 0.01\n",
    "  \n",
    "  vs_from_u = \\\n",
    "  ([[(0, 5), (1, 3), (3, 1)],\n",
    "   [(0, 4), (3, 1)],\n",
    "   [(0, 1), (1, 1), (3, 5)],\n",
    "   [(0, 1), (3, 4)],\n",
    "   [(1, 1), (2, 5), (3, 4)]])\n",
    "  \n",
    "  np.random.seed(0)\n",
    "  \n",
    "  first = []\n",
    "  for i in range(5):\n",
    "    first.append(np.random.rand(2, 1))\n",
    "  second = np.zeros((5, 1))\n",
    "  third = []\n",
    "  for i in range(5):\n",
    "    third.append(np.random.rand(2, 1))\n",
    "  fourth = np.zeros((5, 1))\n",
    "  x0 = (first, second, third, fourth)\n",
    "  \n",
    "  x_result = update_U(ratings_small, vs_from_u, x0, k, lam)\n",
    "  \n",
    "  x_list = [[u.tolist() for u in x_result[0]],\n",
    "            x_result[1].tolist(),\n",
    "            [v.tolist() for v in x_result[2]],\n",
    "            x_result[3].tolist()]\n",
    "  \n",
    "  \n",
    "  assert np.all(np.isclose(x_list[0], np.array([[[4.048442188078757], [-2.5000082235465526]],\n",
    "                                                [[3.2715388359271054], [-1.2879317400952521]],\n",
    "                                                [[-6.237522315961142], [-2.9639103597721355]],\n",
    "                                                [[-3.2715388359271054], [1.2879317400952521]],\n",
    "                                                [[-4.87111151185168], [-1.761023196019822]] ])))\n",
    "  assert np.all(np.isclose(x_list[1], \n",
    "                           np.array([[3.043665230868208], [2.048616799474877], [7.462166369240114], [2.951383200525123], [5.487071919883842]])))\n",
    "  assert np.all(np.isclose(x_list[2], np.array([[[0.7917250380826646], [0.5288949197529045]],\n",
    "                                       [[0.5680445610939323], [0.925596638292661]],\n",
    "                                       [[0.07103605819788694], [0.08712929970154071]],\n",
    "                                       [[0.02021839744032572], [0.832619845547938]],\n",
    "                                       [[0.7781567509498505], [0.8700121482468192]] ])))\n",
    "  assert np.all(np.isclose(x_list[3], np.array([[0.0], [0.0], [0.0], [0.0], [0.0]])))\n",
    "  print(\"Test passed!\")\n",
    "\n",
    "update_U_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS result for k = 2 : rmse train = [[0.00223025]] ; rmse validate = [[0.00223025]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[ 1.22319618],\n",
       "         [-0.84002598]]),\n",
       "  array([[ 0.81430847],\n",
       "         [-0.74411966]]),\n",
       "  array([[-1.23397271],\n",
       "         [ 0.81225434]]),\n",
       "  array([[-0.80628622],\n",
       "         [ 0.73678888]]),\n",
       "  array([[1.18339142],\n",
       "         [4.3807127 ]])],\n",
       " array([ 3.58480402,  2.94624916,  2.3787652 ,  2.01506587, -0.13938456]),\n",
       " [array([[ 0.74896744],\n",
       "         [-0.57491886]]),\n",
       "  array([[0.54666151],\n",
       "         [0.33433815]]),\n",
       "  array([[0.],\n",
       "         [0.]]),\n",
       "  array([[-1.25625301],\n",
       "         [ 1.27742317]])],\n",
       " array([ 0.01424939, -0.97352078,  5.13938456,  0.02929921]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple validate case\n",
    "mf_als(ratings_small, ratings_small,lam=0.01, max_iter=10, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 sgd_step\n",
    "\n",
    "Plug and play the gradient formulas from https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week12/recommender_systems/?child=last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## provided code\n",
    "\n",
    "# The SGD outer loop\n",
    "def mf_sgd(data_train, data_validate, step_size_fn, k=2, lam=0.02, max_iter=100, verbose=False):\n",
    "    # size of the problem\n",
    "    ndata = len(data_train)\n",
    "    n = max(d[0] for d in data_train)+1\n",
    "    m = max(d[1] for d in data_train)+1\n",
    "    # Distribute the lambda among the users and items\n",
    "    lam_uv = lam/counts(data_train,0), lam/counts(data_train,1)\n",
    "    # Initial guess at u, b_u, v, b_v (also b)\n",
    "    x = ([np.random.normal(1/k, size=(k,1)) for j in range(n)],\n",
    "         np.zeros(n),\n",
    "         [np.random.normal(1/k, size=(k,1)) for j in range(m)],\n",
    "         np.zeros(m))\n",
    "    di = int(max_iter/10.)\n",
    "    for i in range(max_iter):\n",
    "        if i%di == 0 and verbose:\n",
    "            print('i=', i, 'train rmse=', rmse(data_train, x),\n",
    "                  'validate rmse', data_validate and rmse(data_validate, x))\n",
    "        step = step_size_fn(i)\n",
    "        j = np.random.randint(ndata)            # pick data item\n",
    "        sgd_step(data_train[j], x, lam_uv, step) # modify x\n",
    "    print('SGD result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## my code\n",
    "\n",
    "def sgd_step(data, x, lam, step):\n",
    "    (a, i, r) = data\n",
    "    (u, b_u, v, b_v) = x\n",
    "    (lam_u, lam_v) = lam\n",
    "    Y_ai = r\n",
    "    v_i = v[i]\n",
    "    u_a = u[a]\n",
    "\n",
    "    u_a_grad = (np.dot(u_a.T,v_i)+b_u[a]+b_v[i]-Y_ai)*v_i + lam_u[a] * u_a\n",
    "    b_u_grad = (np.dot(u_a.T,v_i)+b_u[a]+b_v[i]-Y_ai)\n",
    "\n",
    "    v_i_grad = (np.dot(u_a.T,v_i)+b_u[a]+b_v[i]-Y_ai)*u_a + lam_v[i] * v_i\n",
    "    b_v_grad = (np.dot(u_a.T,v_i)+b_u[a]+b_v[i]-Y_ai)\n",
    "    u[a] = u[a] - step* u_a_grad\n",
    "    b_u[a] = b_u[a] - step* b_u_grad\n",
    "    v[i] = v[i] - step* v_i_grad\n",
    "    b_v[i] = b_v[i] - step* b_v_grad\n",
    "    x = (u,b_u,v,b_v)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "SGD result for k = 2 : rmse train = [[0.00464191]] ; rmse validate = [[0.00464191]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[2.01544792],\n",
       "         [0.3561021 ]]),\n",
       "  array([[1.42507837],\n",
       "         [0.48798664]]),\n",
       "  array([[-1.48635907],\n",
       "         [ 0.07673984]]),\n",
       "  array([[-0.13158296],\n",
       "         [-0.8980307 ]]),\n",
       "  array([[ 2.27448457],\n",
       "         [-1.34763427]])],\n",
       " array([0.36069283, 0.19597829, 1.96549433, 0.44934677, 1.24485534]),\n",
       " [array([[1.55024696],\n",
       "         [0.60202987]]),\n",
       "  array([[0.88126647],\n",
       "         [1.82086226]]),\n",
       "  array([[1.67896045],\n",
       "         [0.37812022]]),\n",
       "  array([[-0.57784711],\n",
       "         [-1.32652495]])],\n",
       " array([1.29609553, 0.2052761 , 0.43746584, 2.27753008]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd_step_test():\n",
    "  '''\n",
    "  This is a test function provided to help you debug your implementation\n",
    "  '''\n",
    "  step = 0.025\n",
    "  lam =(np.array([ 0.00333333,  0.005,  0.00333333,  0.005,  0.00333333]), np.array([ 0.0025,  0.00333333,  0.01,  0.002]))\n",
    "  \n",
    "  np.random.seed(0)\n",
    "  \n",
    "  first = []\n",
    "  for i in range(5):\n",
    "    first.append(np.random.rand(2, 1))\n",
    "  second = np.zeros((5, 1))\n",
    "  third = []\n",
    "  for i in range(5):\n",
    "    third.append(np.random.rand(2, 1))\n",
    "  fourth = np.zeros((5, 1))\n",
    "  x0 = (first, second, third, fourth)\n",
    "  \n",
    "  x_result = sgd_step(ratings_small[3], x0, lam, step)\n",
    "  \n",
    "  x_list = [[u.tolist() for u in x_result[0]],\n",
    "            x_result[1].tolist(),\n",
    "            [v.tolist() for v in x_result[2]],\n",
    "            x_result[3].tolist()]\n",
    "    \n",
    "  assert np.all(np.isclose(x_list[0], np.array([[[0.5488135039273248], [0.7151893663724195]],\n",
    "                                                [[0.6667107015911342], [0.5875840438721468]],\n",
    "                                                [[0.4236547993389047], [0.6458941130666561]],\n",
    "                                                [[0.4375872112626925], [0.8917730007820798]],\n",
    "                                                [[0.9636627605010293], [0.3834415188257777]]])))\n",
    "  assert np.all(np.isclose(x_list[1], np.array([[0.0], [0.08086477989447478], [0.0], [0.0], [0.0]])))\n",
    "  assert np.all(np.isclose(x_list[2], np.array([[[0.8404178830022684], [0.5729237224816648]],\n",
    "                                                [[0.5680445610939323], [0.925596638292661]],\n",
    "                                                [[0.07103605819788694], [0.08712929970154071]],\n",
    "                                                [[0.02021839744032572], [0.832619845547938]],\n",
    "                                                [[0.7781567509498505], [0.8700121482468192]]])))\n",
    "  assert np.all(np.isclose(x_list[3], np.array([[0.08086477989447478], [0.0], [0.0], [0.0], [0.0]])))\n",
    "  print(\"Test passed!\")\n",
    "sgd_step_test()\n",
    "\n",
    "mf_sgd(ratings_small, ratings_small, step_size_fn=lambda i: 0.1,\n",
    "       lam=0.01, max_iter=1000, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) MovieLens\n",
    "\n",
    "Movie ratings compution using ALS \n",
    "dataset from https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS result for k = 2 : rmse train = [[0.00672852]] ; rmse validate = [[0.00672852]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[-0.18386787],\n",
       "         [-2.9655237 ]]),\n",
       "  array([[ 0.7045643 ],\n",
       "         [-1.93463287]]),\n",
       "  array([[-1.64317857],\n",
       "         [ 2.22671523]]),\n",
       "  array([[-0.67642905],\n",
       "         [ 1.85737747]]),\n",
       "  array([[-0.56865292],\n",
       "         [ 2.20820726]])],\n",
       " array([2.59866461, 1.57913883, 4.71547332, 3.37654252, 3.11917172]),\n",
       " [array([[ 1.040202  ],\n",
       "         [-0.88537615]]),\n",
       "  array([[ 1.45491916],\n",
       "         [-0.38127039]]),\n",
       "  array([[0.],\n",
       "         [0.]]),\n",
       "  array([[0.54783477],\n",
       "         [0.5177597 ]])],\n",
       " array([-0.03027794, -0.4625075 ,  1.88082828,  0.03744944]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After retrieving the output x from mf_als, you can use this function to save the output so\n",
    "# you don't have to re-train your model\n",
    "def save_model(x):\n",
    "    pickle.dump(x, open(\"ALSmodel\", \"wb\"))\n",
    "\n",
    "# After training and saving your model once, you can use this function to retrieve the previous model\n",
    "def load_model():\n",
    "    x = pickle.load(open(\"ALSmodel\", \"rb\"))\n",
    "    return x\n",
    "\n",
    "# Compute the root mean square error\n",
    "def rmse(data, x):\n",
    "    error = 0.\n",
    "    for datum in data:\n",
    "        error += (datum[-1] - pred(datum, x))**2\n",
    "    return np.sqrt(error/len(data))\n",
    "\n",
    "# Counts of users and movies, used to calibrate lambda\n",
    "def counts(data, index):\n",
    "    item_count = {}\n",
    "    for datum in data:\n",
    "        j = datum[index]\n",
    "        if j in item_count:\n",
    "            item_count[j] += 1\n",
    "        else:\n",
    "            item_count[j] = 1\n",
    "    c = np.ones(max(item_count.keys())+1)\n",
    "    for i,v in item_count.items(): c[i]=v\n",
    "    return c\n",
    "\n",
    "# The ALS outer loop\n",
    "def mf_als(data_train, data_validate, k=2, lam=0.02, max_iter=100, verbose=False):\n",
    "    # size of the problem\n",
    "    n = max(d[0] for d in data_train)+1 # users\n",
    "    m = max(d[1] for d in data_train)+1 # items\n",
    "    # which entries are set in each row and column\n",
    "    us_from_v = [[] for i in range(m)]\n",
    "    vs_from_u = [[] for a in range(n)]\n",
    "    for (a, i, r) in data_train:\n",
    "        us_from_v[i].append((a, r))\n",
    "        vs_from_u[a].append((i, r))\n",
    "    # Initial guess at u, b_u, v, b_v\n",
    "    # Note that u and v are lists of column vectors (columns of U, V).\n",
    "    x = ([np.random.normal(1/k, size=(k,1)) for a in range(n)],\n",
    "          np.zeros(n),\n",
    "          [np.random.normal(1/k, size=(k,1)) for i in range(m)],\n",
    "          np.zeros(m))\n",
    "    # Alternation, modifies the contents of x\n",
    "    start_time = time.time()\n",
    "    for i in range(max_iter):\n",
    "        update_U(data_train, vs_from_u, x, k, lam)\n",
    "        update_V(data_train, us_from_v, x, k, lam)\n",
    "        if verbose:\n",
    "            print('train rmse', rmse(data_train, x), 'validate rmse', data_validate and rmse(data_validate, x))\n",
    "        if data_validate == None: # code is slower, print out progress\n",
    "            print(\"Iteration {} finished. Total Elapsed Time: {:.2f}\".format(i + 1, time.time() - start_time))\n",
    "    # The root mean square errors measured on validate set\n",
    "    if data_validate != None:\n",
    "        print('ALS result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
    "    return x\n",
    "\n",
    "# Simple validate case\n",
    "mf_als(ratings_small, ratings_small,lam=0.01, max_iter=10, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## provided code\n",
    "\n",
    "def load_ratings_data_small(path_data='ratings.csv'):\n",
    "    \"\"\"\n",
    "    Returns two lists of triples (a, i, r) (training, validate)\n",
    "    \"\"\"\n",
    "    # we want to \"randomly\" sample but make it deterministic\n",
    "    def user_hash(uid):\n",
    "        return 71 * uid % 401\n",
    "    def user_movie_hash(uid, iid):\n",
    "        return (17 * uid + 43 * iid) % 61\n",
    "    data_train = []\n",
    "    data_validate = []\n",
    "    with open(path_data) as f_data:\n",
    "        for line in f_data:\n",
    "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
    "            h1 = user_hash(int(uid))\n",
    "            if h1 <= 40:\n",
    "                h2 = user_movie_hash(int(uid), int(iid))\n",
    "                if h2 <= 12:\n",
    "                    data_validate.append([int(uid), int(iid), float(rating)])\n",
    "                else:\n",
    "                    data_train.append([int(uid), int(iid), float(rating)])\n",
    "    print('Loading from', path_data,\n",
    "          'users_train', len(set(x[0] for x in data_train)),\n",
    "          'items_train', len(set(x[1] for x in data_train)),\n",
    "          'users_validate', len(set(x[0] for x in data_validate)),\n",
    "          'items_validate', len(set(x[1] for x in data_validate)))\n",
    "    return data_train, data_validate\n",
    "\n",
    "def load_ratings_data(path_data='ratings.csv'):\n",
    "    \"\"\"\n",
    "    Returns a list of triples (a, i, r)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path_data) as f_data:\n",
    "        for line in f_data:\n",
    "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
    "            data.append([int(uid), int(iid), float(rating)])\n",
    "\n",
    "    print('Loading from', path_data,\n",
    "          'users', len(set(x[0] for x in data)),\n",
    "          'items', len(set(x[1] for x in data)))\n",
    "    return data\n",
    "\n",
    "def load_movies(path_movies='movies.csv'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping item_id to item_name and another dictionary\n",
    "    mapping item_id to a list of genres\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    genreMap = {}\n",
    "    with open(path_movies, encoding = \"utf8\") as f_data:\n",
    "        for line in f_data:\n",
    "            parts = line.strip().split(\",\")\n",
    "            item_id = int(parts[0])\n",
    "            item_name = \",\".join(parts[1:-1]) # file is poorly formatted\n",
    "            item_genres = parts[-1].split(\"|\")\n",
    "            data[item_id] = item_name\n",
    "            genreMap[item_id] = item_genres\n",
    "    return data, genreMap\n",
    "\n",
    "def baseline(train, validate):\n",
    "    item_sum = {}\n",
    "    item_count = {}\n",
    "    total = 0\n",
    "    for (i, j, r) in train:\n",
    "        total += r\n",
    "        if j in item_sum:\n",
    "            item_sum[j] += 3\n",
    "            item_count[j] += 1\n",
    "        else:\n",
    "            item_sum[j] = r\n",
    "            item_count[j] = 1\n",
    "    error = 0\n",
    "    avg = total/len(train)\n",
    "    for (i, j, r) in validate:\n",
    "        pred = item_sum[j]/item_count[j] if j in item_count else avg\n",
    "        error += (r - pred)**2\n",
    "    return np.sqrt(error/len(validate))\n",
    "\n",
    "# Load the movie data\n",
    "# Below is code for the smaller dataset, used in section 3 of the HW\n",
    "def tuning_als(max_iter_als=20, verbose=False):\n",
    "    b1, v1 = load_ratings_data_small()\n",
    "    print('Baseline rmse (predict item average)', baseline(b1, v1))\n",
    "    print('Running on the MovieLens data')\n",
    "    lams = [0.1,1,10,100]\n",
    "    ks = [1,2,3]\n",
    "    for k in ks:\n",
    "        for lam in lams:\n",
    "            print('ALS, k =', k, 'and lam', lam)\n",
    "            mf_als(b1, v1, lam = lam, max_iter=max_iter_als, k=k, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### begin my code\n",
    "\n",
    "#### 4.1) Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ratings.csv users 13366 items 2000\n",
      "Iteration 1 finished. Total Elapsed Time: 12.01\n",
      "Iteration 2 finished. Total Elapsed Time: 23.89\n",
      "Iteration 3 finished. Total Elapsed Time: 35.82\n",
      "Iteration 4 finished. Total Elapsed Time: 47.74\n",
      "Iteration 5 finished. Total Elapsed Time: 59.90\n",
      "Iteration 6 finished. Total Elapsed Time: 71.75\n",
      "Iteration 7 finished. Total Elapsed Time: 83.65\n",
      "Iteration 8 finished. Total Elapsed Time: 95.56\n",
      "Iteration 9 finished. Total Elapsed Time: 107.51\n",
      "Iteration 10 finished. Total Elapsed Time: 119.65\n",
      "Iteration 11 finished. Total Elapsed Time: 132.25\n",
      "Iteration 12 finished. Total Elapsed Time: 144.86\n",
      "Iteration 13 finished. Total Elapsed Time: 157.09\n",
      "Iteration 14 finished. Total Elapsed Time: 168.85\n",
      "Iteration 15 finished. Total Elapsed Time: 180.56\n",
      "Iteration 16 finished. Total Elapsed Time: 192.56\n",
      "Iteration 17 finished. Total Elapsed Time: 205.25\n",
      "Iteration 18 finished. Total Elapsed Time: 217.12\n",
      "Iteration 19 finished. Total Elapsed Time: 229.02\n",
      "Iteration 20 finished. Total Elapsed Time: 241.46\n"
     ]
    }
   ],
   "source": [
    "# No need to run a second time - use saved model instead (see next cell)\n",
    "\n",
    "data = load_ratings_data()\n",
    "movies_dict, genres_dict = load_movies()\n",
    "model = mf_als(data, None, k=10, lam=1, max_iter=20)\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "### homework code to see the user data.  I should use pandas instead\n",
    "\n",
    "def get_user_data(data, user):\n",
    "    user_data = []\n",
    "    for d in data:\n",
    "        if d[0] == user:\n",
    "            user_data.append(d)\n",
    "    return user_data\n",
    "\n",
    "def display_user_data(user_data,movies_dict,genres_dict, min_rating, genre_filter='None'):\n",
    "    if genre_filter == None:\n",
    "        use_filter = False\n",
    "    else:\n",
    "        use_filter = True\n",
    "    print(\"Movie# \\tRating\\tGenres\")\n",
    "\n",
    "    \n",
    "    for d in user_data:\n",
    "        (user, movie, rating) = d\n",
    "        genres=genres_dict[movie]\n",
    "        movie = movies_dict[movie]\n",
    "        if use_filter == True:\n",
    "            if genre_filter not in genres:\n",
    "                pass\n",
    "            elif rating >= min_rating:\n",
    "                print(f\"{movie}\\n{rating}\\t{genres}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) User Data\n",
    "\n",
    "By running the code below, we can see the list of films that the user has rated 5, and the genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270894, 1148, 5.0], [270894, 661, 5.0], [270894, 2096, 5.0], [270894, 32031, 5.0], [270894, 4006, 5.0], [270894, 71264, 5.0], [270894, 2018, 5.0], [270894, 56152, 5.0], [270894, 8907, 5.0], [270894, 112175, 5.0], [270894, 2700, 5.0], [270894, 673, 5.0], [270894, 97913, 5.0], [270894, 8961, 5.0], [270894, 4873, 5.0], [270894, 3034, 5.0], [270894, 5690, 5.0], [270894, 8965, 5.0], [270894, 2141, 5.0], [270894, 2090, 5.0], [270894, 595, 5.0], [270894, 594, 5.0], [270894, 50872, 5.0], [270894, 59784, 5.0], [270894, 45517, 5.0], [270894, 2987, 5.0], [270894, 27660, 5.0], [270894, 2078, 5.0], [270894, 81564, 5.0], [270894, 364, 5.0], [270894, 27904, 5.0], [270894, 3159, 5.0], [270894, 134853, 5.0], [270894, 37729, 5.0], [270894, 5971, 5.0], [270894, 103335, 5.0], [270894, 8360, 5.0], [270894, 3751, 5.0], [270894, 3000, 5.0], [270894, 1907, 5.0], [270894, 709, 5.0], [270894, 6283, 5.0], [270894, 1032, 5.0], [270894, 1030, 5.0], [270894, 2687, 5.0], [270894, 72226, 5.0], [270894, 60069, 5.0], [270894, 4446, 5.0], [270894, 1025, 5.0], [270894, 6350, 5.0], [270894, 1022, 5.0], [270894, 1029, 5.0], [270894, 53121, 5.0], [270894, 2355, 5.0], [270894, 741, 5.0], [270894, 3114, 5.0], [270894, 4016, 5.0], [270894, 76093, 5.0], [270894, 239, 5.0], [270894, 5218, 5.0], [270894, 2137, 5.0], [270894, 38038, 5.0], [270894, 48, 5.0], [270894, 2139, 5.0], [270894, 1, 5.0], [270894, 81847, 5.0], [270894, 55442, 5.0], [270894, 610, 5.0], [270894, 2857, 5.0], [270894, 2123, 5.0], [270894, 2080, 5.0], [270894, 2081, 5.0], [270894, 108932, 5.0], [270894, 8917, 5.0], [270894, 4022, 4.0], [270894, 344, 3.5], [270894, 3998, 2.5], [270894, 292, 3.5], [270894, 45447, 3.0], [270894, 41285, 4.0], [270894, 2688, 2.0], [270894, 5293, 2.5], [270894, 1997, 3.5], [270894, 112556, 4.0], [270894, 553, 3.5], [270894, 1197, 5.0], [270894, 7458, 3.0], [270894, 1379, 3.0], [270894, 4728, 3.0], [270894, 2302, 4.0], [270894, 3744, 4.0], [270894, 3740, 5.0], [270894, 2278, 2.5], [270894, 170, 3.0], [270894, 2053, 2.0], [270894, 2054, 2.0], [270894, 2058, 3.0], [270894, 653, 3.5], [270894, 2699, 3.5], [270894, 6383, 2.5], [270894, 5445, 4.5], [270894, 8622, 1.0], [270894, 412, 5.0], [270894, 2193, 3.5], [270894, 2423, 4.0], [270894, 164, 4.0], [270894, 160, 2.0], [270894, 139644, 4.0], [270894, 3683, 2.0], [270894, 80489, 2.5], [270894, 1974, 4.0], [270894, 1552, 5.0], [270894, 2997, 3.0], [270894, 535, 5.0], [270894, 534, 5.0], [270894, 533, 2.0], [270894, 2993, 3.5], [270894, 224, 3.0], [270894, 2432, 3.0], [270894, 63072, 3.5], [270894, 60074, 3.0], [270894, 2505, 2.5], [270894, 4993, 4.0], [270894, 8861, 2.5], [270894, 104841, 4.0], [270894, 5464, 2.0], [270894, 94777, 3.5], [270894, 427, 3.0], [270894, 1127, 5.0], [270894, 1019, 3.5], [270894, 5673, 1.5], [270894, 435, 2.0], [270894, 231, 4.0], [270894, 232, 2.5], [270894, 4887, 3.0], [270894, 2088, 3.0], [270894, 2082, 3.5], [270894, 2530, 2.0], [270894, 2712, 3.5], [270894, 46530, 3.0], [270894, 2153, 1.5], [270894, 1957, 2.5], [270894, 1956, 4.5], [270894, 1221, 4.0], [270894, 1228, 3.5], [270894, 4343, 2.0], [270894, 2, 3.0], [270894, 69526, 3.5], [270894, 5064, 1.0], [270894, 58025, 3.0], [270894, 60, 3.0], [270894, 3006, 4.0], [270894, 7156, 4.0], [270894, 1373, 2.5], [270894, 1212, 4.0], [270894, 1218, 4.5], [270894, 4370, 1.5], [270894, 1729, 1.0], [270894, 1104, 4.5], [270894, 2398, 4.0], [270894, 4890, 1.0], [270894, 81932, 3.0], [270894, 2600, 4.0], [270894, 1206, 3.5], [270894, 4369, 1.5], [270894, 3100, 3.5], [270894, 3101, 4.0], [270894, 2541, 3.0], [270894, 2380, 2.0], [270894, 905, 3.5], [270894, 909, 3.5], [270894, 7346, 2.5], [270894, 7347, 2.5], [270894, 3257, 3.0], [270894, 3450, 3.5], [270894, 41997, 2.5], [270894, 2949, 2.0], [270894, 1690, 2.0], [270894, 1301, 4.0], [270894, 494, 5.0], [270894, 2375, 2.5], [270894, 6287, 2.5], [270894, 1086, 5.0], [270894, 2118, 4.0], [270894, 2112, 3.0], [270894, 2021, 2.0], [270894, 353, 4.0], [270894, 79, 2.5], [270894, 1262, 5.0], [270894, 1267, 2.0], [270894, 4054, 2.0], [270894, 88129, 3.0], [270894, 5026, 4.5], [270894, 4447, 2.5], [270894, 6296, 4.0], [270894, 2109, 3.5], [270894, 53125, 4.0], [270894, 3439, 2.0], [270894, 6873, 3.0], [270894, 1253, 2.5], [270894, 3959, 2.5], [270894, 5943, 2.5], [270894, 40819, 4.0], [270894, 61240, 2.5], [270894, 2641, 3.0], [270894, 3424, 5.0], [270894, 2005, 4.0], [270894, 662, 3.0], [270894, 44199, 4.0], [270894, 1408, 3.0], [270894, 4214, 3.0], [270894, 1244, 5.0], [270894, 3968, 3.0], [270894, 3704, 3.5], [270894, 942, 2.0], [270894, 2657, 3.0], [270894, 102903, 5.0], [270894, 586, 3.5], [270894, 585, 1.0], [270894, 3152, 5.0], [270894, 3156, 2.5], [270894, 94864, 5.0], [270894, 1500, 4.0], [270894, 3088, 2.0], [270894, 52281, 3.5], [270894, 4718, 2.5], [270894, 51662, 3.5], [270894, 54001, 2.5], [270894, 52, 4.0], [270894, 3408, 1.0], [270894, 53000, 3.0], [270894, 2060, 1.5], [270894, 33493, 4.0], [270894, 1645, 4.0], [270894, 1643, 4.0], [270894, 3363, 2.5], [270894, 4270, 1.5], [270894, 464, 3.0], [270894, 1356, 5.0], [270894, 1518, 3.0], [270894, 55280, 4.0], [270894, 3983, 5.0], [270894, 265, 4.0], [270894, 1215, 5.0]]\n",
      "Movie# \tRating\tGenres\n",
      "Wallace & Gromit: The Wrong Trousers (1993)\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Crime']\n",
      "\n",
      "James and the Giant Peach (1996)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Fantasy', 'Musical']\n",
      "\n",
      "Sleeping Beauty (1959)\n",
      "5.0\t['Animation', 'Children', 'Musical']\n",
      "\n",
      "Robots (2005)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy', 'Sci-Fi', 'IMAX']\n",
      "\n",
      "Transformers: The Movie (1986)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Sci-Fi']\n",
      "\n",
      "Cloudy with a Chance of Meatballs (2009)\n",
      "5.0\t['Animation', 'Children', 'Fantasy', 'IMAX']\n",
      "\n",
      "Bambi (1942)\n",
      "5.0\t['Animation', 'Children', 'Drama']\n",
      "\n",
      "Enchanted (2007)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy', 'Musical', 'Romance']\n",
      "\n",
      "Shark Tale (2004)\n",
      "5.0\t['Animation', 'Children', 'Comedy']\n",
      "\n",
      "How to Train Your Dragon 2 (2014)\n",
      "5.0\t['Action', 'Adventure', 'Animation']\n",
      "\n",
      "\"South Park: Bigger, Longer and Uncut (1999)\"\n",
      "5.0\t['Animation', 'Comedy', 'Musical']\n",
      "\n",
      "Space Jam (1996)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy', 'Sci-Fi']\n",
      "\n",
      "Wreck-It Ralph (2012)\n",
      "5.0\t['Animation', 'Comedy']\n",
      "\n",
      "\"Incredibles, The (2004)\"\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "Waking Life (2001)\n",
      "5.0\t['Animation', 'Drama', 'Fantasy']\n",
      "\n",
      "Robin Hood (1973)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Musical']\n",
      "\n",
      "Grave of the Fireflies (Hotaru no haka) (1988)\n",
      "5.0\t['Animation', 'Drama', 'War']\n",
      "\n",
      "\"Polar Express, The (2004)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Fantasy', 'IMAX']\n",
      "\n",
      "\"American Tail, An (1986)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "\"Rescuers, The (1977)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Crime', 'Drama']\n",
      "\n",
      "Beauty and the Beast (1991)\n",
      "5.0\t['Animation', 'Children', 'Fantasy', 'Musical', 'Romance', 'IMAX']\n",
      "\n",
      "Snow White and the Seven Dwarfs (1937)\n",
      "5.0\t['Animation', 'Children', 'Drama', 'Fantasy', 'Musical']\n",
      "\n",
      "Ratatouille (2007)\n",
      "5.0\t['Animation', 'Children', 'Drama']\n",
      "\n",
      "Kung Fu Panda (2008)\n",
      "5.0\t['Action', 'Animation', 'Children', 'Comedy', 'IMAX']\n",
      "\n",
      "Cars (2006)\n",
      "5.0\t['Animation', 'Children', 'Comedy']\n",
      "\n",
      "Who Framed Roger Rabbit? (1988)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Fantasy', 'Mystery']\n",
      "\n",
      "\"Animatrix, The (2003)\"\n",
      "5.0\t['Action', 'Animation', 'Drama', 'Sci-Fi']\n",
      "\n",
      "\"Jungle Book, The (1967)\"\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Musical']\n",
      "\n",
      "Megamind (2010)\n",
      "5.0\t['Action', 'Animation', 'Children', 'Comedy', 'Sci-Fi', 'IMAX']\n",
      "\n",
      "\"Lion King, The (1994)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Drama', 'Musical', 'IMAX']\n",
      "\n",
      "\"Scanner Darkly, A (2006)\"\n",
      "5.0\t['Animation', 'Drama', 'Mystery', 'Sci-Fi', 'Thriller']\n",
      "\n",
      "Fantasia 2000 (1999)\n",
      "5.0\t['Animation', 'Children', 'Musical', 'IMAX']\n",
      "\n",
      "Inside Out (2015)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Drama', 'Fantasy']\n",
      "\n",
      "Corpse Bride (2005)\n",
      "5.0\t['Animation', 'Comedy', 'Fantasy', 'Musical', 'Romance']\n",
      "\n",
      "My Neighbor Totoro (Tonari no Totoro) (1988)\n",
      "5.0\t['Animation', 'Children', 'Drama', 'Fantasy']\n",
      "\n",
      "Despicable Me 2 (2013)\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'IMAX']\n",
      "\n",
      "Shrek 2 (2004)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Musical', 'Romance']\n",
      "\n",
      "Chicken Run (2000)\n",
      "5.0\t['Animation', 'Children', 'Comedy']\n",
      "\n",
      "Princess Mononoke (Mononoke-hime) (1997)\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Drama', 'Fantasy']\n",
      "\n",
      "Mulan (1998)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Drama', 'Musical', 'Romance']\n",
      "\n",
      "Oliver & Company (1988)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Musical']\n",
      "\n",
      "Cowboy Bebop: The Movie (Cowboy Bebop: Tengoku no Tobira) (2001)\n",
      "5.0\t['Action', 'Animation', 'Sci-Fi', 'Thriller']\n",
      "\n",
      "Alice in Wonderland (1951)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Fantasy', 'Musical']\n",
      "\n",
      "Pete's Dragon (1977)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Musical']\n",
      "\n",
      "Tarzan (1999)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Drama']\n",
      "\n",
      "Fantastic Mr. Fox (2009)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Crime']\n",
      "\n",
      "WALL·E (2008)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Romance', 'Sci-Fi']\n",
      "\n",
      "Final Fantasy: The Spirits Within (2001)\n",
      "5.0\t['Adventure', 'Animation', 'Fantasy', 'Sci-Fi']\n",
      "\n",
      "\"Sword in the Stone, The (1963)\"\n",
      "5.0\t['Animation', 'Children', 'Fantasy', 'Musical']\n",
      "\n",
      "Laputa: Castle in the Sky (Tenkû no shiro Rapyuta) (1986)\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Children', 'Fantasy', 'Sci-Fi']\n",
      "\n",
      "Cinderella (1950)\n",
      "5.0\t['Animation', 'Children', 'Fantasy', 'Musical', 'Romance']\n",
      "\n",
      "Dumbo (1941)\n",
      "5.0\t['Animation', 'Children', 'Drama', 'Musical']\n",
      "\n",
      "Shrek the Third (2007)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "\"Bug's Life, A (1998)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "Ghost in the Shell (Kôkaku kidôtai) (1995)\n",
      "5.0\t['Animation', 'Sci-Fi']\n",
      "\n",
      "Toy Story 2 (1999)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "\"Emperor's New Groove, The (2000)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "How to Train Your Dragon (2010)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Fantasy', 'IMAX']\n",
      "\n",
      "\"Goofy Movie, A (1995)\"\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Romance']\n",
      "\n",
      "Ice Age (2002)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "Charlotte's Web (1973)\n",
      "5.0\t['Animation', 'Children']\n",
      "\n",
      "Wallace & Gromit in The Curse of the Were-Rabbit (2005)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "Pocahontas (1995)\n",
      "5.0\t['Animation', 'Children', 'Drama', 'Musical', 'Romance']\n",
      "\n",
      "\"Secret of NIMH, The (1982)\"\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Drama']\n",
      "\n",
      "Toy Story (1995)\n",
      "5.0\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "Tangled (2010)\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Fantasy', 'Musical', 'Romance', 'IMAX']\n",
      "\n",
      "Persepolis (2007)\n",
      "5.0\t['Animation', 'Drama']\n",
      "\n",
      "Heavy Metal (1981)\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Horror', 'Sci-Fi']\n",
      "\n",
      "Yellow Submarine (1968)\n",
      "5.0\t['Adventure', 'Animation', 'Comedy', 'Fantasy', 'Musical']\n",
      "\n",
      "All Dogs Go to Heaven (1989)\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Drama', 'Fantasy']\n",
      "\n",
      "Lady and the Tramp (1955)\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Romance']\n",
      "\n",
      "\"Little Mermaid, The (1989)\"\n",
      "5.0\t['Animation', 'Children', 'Comedy', 'Musical', 'Romance']\n",
      "\n",
      "The Lego Movie (2014)\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "Team America: World Police (2004)\n",
      "5.0\t['Action', 'Adventure', 'Animation', 'Comedy']\n",
      "\n",
      "\"Princess Bride, The (1987)\"\n",
      "5.0\t['Action', 'Adventure', 'Comedy', 'Fantasy', 'Romance']\n",
      "\n",
      "Big Trouble in Little China (1986)\n",
      "5.0\t['Action', 'Adventure', 'Comedy', 'Fantasy']\n",
      "\n",
      "\"Age of Innocence, The (1993)\"\n",
      "5.0\t['Drama']\n",
      "\n",
      "Con Air (1997)\n",
      "5.0\t['Action', 'Adventure', 'Thriller']\n",
      "\n",
      "Short Cuts (1993)\n",
      "5.0\t['Drama']\n",
      "\n",
      "Shadowlands (1993)\n",
      "5.0\t['Drama', 'Romance']\n",
      "\n",
      "\"Abyss, The (1989)\"\n",
      "5.0\t['Action', 'Adventure', 'Sci-Fi', 'Thriller']\n",
      "\n",
      "Executive Decision (1996)\n",
      "5.0\t['Action', 'Adventure', 'Thriller']\n",
      "\n",
      "Dial M for Murder (1954)\n",
      "5.0\t['Crime', 'Mystery', 'Thriller']\n",
      "\n",
      "\"Great Escape, The (1963)\"\n",
      "5.0\t['Action', 'Adventure', 'Drama', 'War']\n",
      "\n",
      "Do the Right Thing (1989)\n",
      "5.0\t['Drama']\n",
      "\n",
      "Manhattan (1979)\n",
      "5.0\t['Comedy', 'Drama', 'Romance']\n",
      "\n",
      "Now You See Me (2013)\n",
      "5.0\t['Crime', 'Mystery', 'Thriller']\n",
      "\n",
      "\"Last Picture Show, The (1971)\"\n",
      "5.0\t['Drama']\n",
      "\n",
      "Prometheus (2012)\n",
      "5.0\t['Action', 'Horror', 'Sci-Fi', 'IMAX']\n",
      "\n",
      "Star Trek: First Contact (1996)\n",
      "5.0\t['Action', 'Adventure', 'Sci-Fi', 'Thriller']\n",
      "\n",
      "You Can Count on Me (2000)\n",
      "5.0\t['Drama', 'Romance']\n",
      "\n",
      "Army of Darkness (1993)\n",
      "5.0\t['Action', 'Adventure', 'Comedy', 'Fantasy', 'Horror']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "guy = get_user_data(data, 270894)\n",
    "print(guy)\n",
    "display_user_data(guy,movies_dict,genres_dict, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for predicting ratings for each of the movies he hasn't seen yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies seen:  244 \n",
      "\n",
      "Number of total movies: 2000 \n",
      "\n",
      "Calculating predicted ratings: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_all_movies(user_data,model, movies_dict):\n",
    "    (u, b_u, v, b_v) = model\n",
    "    movies_seen = []\n",
    "    recommended_movies = []\n",
    "    movie_list = np.array([[],[]])\n",
    "    for d in user_data:\n",
    "        (user, movie, rating) = d\n",
    "        movies_seen.append(movie)\n",
    "    print(\"Movies seen: \",len(movies_seen),\"\\n\")\n",
    "    print(\"Number of total movies:\", len(movies_dict), \"\\n\")\n",
    "    print(\"Calculating predicted ratings: \\n\")\n",
    "    for i in movies_dict:\n",
    "        if i not in movies_seen:\n",
    "            predicted = pred((user,i,1),model)[0,0]\n",
    "            #print(predicted)\n",
    "            movie_list = np.hstack((movie_list,np.array([[i],[predicted]])))\n",
    "    return movie_list\n",
    "\n",
    "len(model)\n",
    "movie_list = eval_all_movies(guy,model,movies_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here is for sorting and selecting the top 50 recommended films.  I know this is slow to literally make a list of all the movies, sort it, and then truncate it.\n",
    "\n",
    "I think the ideal solution would probably be to implement a stack or something instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indexes = np.argsort(movie_list[1:],axis=1)\n",
    "top_50 = np.squeeze(sorted_indexes[0][-50:])\n",
    "new_list = movie_list[:,top_50]\n",
    "\"\"\" sanity check debugging\n",
    "first_val = movie_list[1,sorted_indexes[0][0]]\n",
    "second_val = movie_list[1,sorted_indexes[0][-1]]\n",
    "print(\"is \", first_val, \" greater than \", second_val, \"?\")\n",
    "\"\"\"\n",
    "#print(new_list[0,-1],new_list[1,-1])\n",
    "#print(len(new_list),\"out of\",len(genres_dict))\n",
    "suggested_data = []\n",
    "for counter in range(50):\n",
    "    suggested_data.append((270894,new_list[0,-counter-1],new_list[1,-counter-1]))\n",
    "    #print(new_list[1,-counter-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top films, filtered by his favorite genre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie# \tRating\tGenres\n",
      "Wallace & Gromit: A Close Shave (1995)\n",
      "5.398315526225244\t['Animation', 'Children', 'Comedy']\n",
      "\n",
      "Finding Nemo (2003)\n",
      "5.3494063110595995\t['Adventure', 'Animation', 'Children', 'Comedy']\n",
      "\n",
      "Up (2009)\n",
      "5.324664682720811\t['Adventure', 'Animation', 'Children', 'Drama']\n",
      "\n",
      "Toy Story 3 (2010)\n",
      "5.236669811580798\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy', 'IMAX']\n",
      "\n",
      "Big Hero 6 (2014)\n",
      "5.226843319560599\t['Action', 'Animation', 'Comedy']\n",
      "\n",
      "\"Monsters, Inc. (2001)\"\n",
      "5.1960174762482545\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy']\n",
      "\n",
      "\"Grand Day Out with Wallace and Gromit, A (1989)\"\n",
      "5.15650670551228\t['Adventure', 'Animation', 'Children', 'Comedy', 'Sci-Fi']\n",
      "\n",
      "Wallace & Gromit: The Best of Aardman Animation (1996)\n",
      "5.12986858656058\t['Adventure', 'Animation', 'Comedy']\n",
      "\n",
      "Aladdin (1992)\n",
      "5.112769819573992\t['Adventure', 'Animation', 'Children', 'Comedy', 'Musical']\n",
      "\n",
      "Howl's Moving Castle (Hauru no ugoku shiro) (2004)\n",
      "5.025363624554047\t['Adventure', 'Animation', 'Fantasy', 'Romance']\n",
      "\n",
      "\"Fox and the Hound, The (1981)\"\n",
      "5.0244670548386425\t['Animation', 'Children', 'Drama']\n",
      "\n",
      "Shrek (2001)\n",
      "5.005319437984704\t['Adventure', 'Animation', 'Children', 'Comedy', 'Fantasy', 'Romance']\n",
      "\n",
      "Spirited Away (Sen to Chihiro no kamikakushi) (2001)\n",
      "4.882591630128367\t['Adventure', 'Animation', 'Fantasy']\n",
      "\n",
      "Despicable Me (2010)\n",
      "4.8608465206687015\t['Animation', 'Children', 'Comedy', 'Crime']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_user_data(suggested_data,movies_dict,genres_dict, 1, genre_filter='Animation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Similarity\n",
    "\n",
    "Finding the closest films to each other based on their Vi vectors in the model.\n",
    "\n",
    "We do this by computing CosSim = dot(v_a,v_b) / (norm(v_a)*norm(v_b)) - this will be proportional to the cosine between the two vectors, meaning that pairs of vectors with bigger CosSim magnitudes will be more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def magnitude(vector): #taken from google\n",
    "    return math.sqrt(sum(pow(element,2) for element in vector))\n",
    "\n",
    "def CosSim(base_movie, movies_dict, model, n):\n",
    "    (u, b_u, v, b_v) = model\n",
    "    v_a = v[base_movie]\n",
    "    print(v_a.shape)\n",
    "    all_cs = np.array([[],[]]) #yes, I know this is lazy.  I have lots of memory right now.\n",
    "    \n",
    "    for movie in movies_dict:\n",
    "        if movie != base_movie:\n",
    "            v_b = v[movie]\n",
    "            cs = np.dot(v_a.T,v_b) / (magnitude(v_a) * magnitude(v_b))\n",
    "            cs = cs[0,0]\n",
    "            all_cs = np.hstack((all_cs,np.array([[movie], [cs]])))\n",
    "    indexes = np.argsort(all_cs[1,:],axis=0)\n",
    "    top_n = all_cs[0,indexes[-n:]]\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "[   329.   3471.   1127.   2115.   2628. 122886.   1198.   1291.   1210.\n",
      "   1196.]\n",
      "Star Trek: Generations (1994)\n",
      "Close Encounters of the Third Kind (1977)\n",
      "\"Abyss, The (1989)\"\n",
      "Indiana Jones and the Temple of Doom (1984)\n",
      "Star Wars: Episode I - The Phantom Menace (1999)\n",
      "Star Wars: Episode VII - The Force Awakens (2015)\n",
      "Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n",
      "Indiana Jones and the Last Crusade (1989)\n",
      "Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "(10, 1)\n",
      "[ 3615.   260.  1196. 51662. 59615.  6934.  1210.  6365.  5378. 33493.]\n",
      "Dinosaur (2000)\n",
      "Star Wars: Episode IV - A New Hope (1977)\n",
      "Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "300 (2007)\n",
      "Indiana Jones and the Kingdom of the Crystal Skull (2008)\n",
      "\"Matrix Revolutions, The (2003)\"\n",
      "Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "\"Matrix Reloaded, The (2003)\"\n",
      "Star Wars: Episode II - Attack of the Clones (2002)\n",
      "Star Wars: Episode III - Revenge of the Sith (2005)\n"
     ]
    }
   ],
   "source": [
    "NewHopeID = 260\n",
    "PhantomID = 2628\n",
    "similar_list = CosSim(NewHopeID,movies_dict,model,10)\n",
    "print(similar_list)\n",
    "for movie in similar_list:\n",
    "    print(movies_dict[movie])\n",
    "similar_phantom = CosSim(PhantomID,movies_dict,model,10)\n",
    "print(similar_phantom)\n",
    "for movie in similar_phantom:\n",
    "    print(movies_dict[movie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2B)\n",
    "\n",
    "The following movies are most similar to A New Hope:\n",
    "\n",
    "[   329.   3471.   1127.   2115.   2628. 122886.   1198.   1291.   1210.\n",
    "   1196.]\n",
    "Star Trek: Generations (1994)\n",
    "\n",
    "Close Encounters of the Third Kind (1977)\n",
    "\n",
    "\"Abyss, The (1989)\"\n",
    "\n",
    "Indiana Jones and the Temple of Doom (1984)\n",
    "\n",
    "Star Wars: Episode I - The Phantom Menace (1999)\n",
    "\n",
    "Star Wars: Episode VII - The Force Awakens (2015)\n",
    "\n",
    "Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n",
    "\n",
    "Indiana Jones and the Last Crusade (1989)\n",
    "\n",
    "Star Wars: Episode VI - Return of the Jedi (1983)\n",
    "\n",
    "Star Wars: Episode V - The Empire Strikes Back (1980)\n",
    "\n",
    "### 4.2C)\n",
    "\n",
    "For Phantom Menace:\n",
    "[ 3615.   260.  1196. 51662. 59615.  6934.  1210.  6365.  5378. 33493.]\n",
    "\n",
    "Dinosaur (2000)\n",
    "\n",
    "Star Wars: Episode IV - A New Hope (1977)\n",
    "\n",
    "Star Wars: Episode V - The Empire Strikes Back (1980)\n",
    "\n",
    "300 (2007)\n",
    "\n",
    "Indiana Jones and the Kingdom of the Crystal Skull (2008)\n",
    "\n",
    "\"Matrix Revolutions, The (2003)\"\n",
    "\n",
    "Star Wars: Episode VI - Return of the Jedi (1983)\n",
    "\n",
    "\"Matrix Reloaded, The (2003)\"\n",
    "\n",
    "Star Wars: Episode II - Attack of the Clones (2002)\n",
    "\n",
    "Star Wars: Episode III - Revenge of the Sith (2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2D-G\n",
    "\n",
    "Now we look for broader similarities between and within genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "Average similarity between unique movie pairs: [[0.0077457]]\n"
     ]
    }
   ],
   "source": [
    "def Similarity(movie1, movie2, model):\n",
    "    (u, b_u, v, b_v) = model\n",
    "    v_a = v[movie1]\n",
    "    v_b = v[movie2]\n",
    "    return np.dot(v_a.T,v_b) / (magnitude(v_a) * magnitude(v_b))\n",
    "\n",
    "key_view = movies_dict.keys()\n",
    "moviekeys = []\n",
    "for movie in key_view:\n",
    "    moviekeys.append(movie)\n",
    "\n",
    "running_sum = 0\n",
    "running_count = 0\n",
    "for i in range(len(movies_dict)-1):\n",
    "    for j in range(len(movies_dict)-i-1):\n",
    "        running_sum = running_sum + Similarity(moviekeys[i],moviekeys[i+j+1], model)\n",
    "        running_count +=1\n",
    "    if i in [200,450,750,1100]: # 900x900 pairs left, 1250x1250 pairs left, 1550x1550 pairs left, 1800x1800 pairs left.  roughly every 20% completion\n",
    "        print(\"**\")\n",
    "print(f\"Average similarity between unique movie pairs: {running_sum/running_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity between unique movie pairs: [[0.0077457]]\n"
     ]
    }
   ],
   "source": [
    "len(movies_dict)\n",
    "range(len(movies_dict)-1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "#i'm sure there's an elegant way to do for (movie, movie) in (movies, movies)\n",
    "#but i'm doing it the simple way that I know\n",
    "\n",
    "#i'll compare my solution with the hw solution afterwards\n",
    "\n",
    "similarity_dict = {}\n",
    "for genre in genres: #19 genres\n",
    "    similarity_dict[genre] = np.zeros(2) # one column for sum and one column for count\n",
    "n = len(movies_dict) \n",
    "for i in range(n-1): #from 0 to 1998\n",
    "    genres_i = genres_dict[moviekeys[i]] #get genres of i'th movie\n",
    "    for j in range(n-i-1): #from 1999 to 1\n",
    "        genres_j = genres_dict[moviekeys[i+j+1]] # get genre of j'th movie after i\n",
    "        for g in genres_i:\n",
    "            if g in genres_j:\n",
    "                similarity_dict[g][0] = similarity_dict[g][0] + Similarity(moviekeys[i],moviekeys[i+j+1],model)\n",
    "                similarity_dict[g][1] = similarity_dict[g][1] + 1\n",
    "    if i in [200,450,750,1100]: # 900x900 pairs left, 1250x1250 pairs left, 1550x1550 pairs left, 1800x1800 pairs left.  roughly every 20% completion\n",
    "        print(\"**\")\n",
    "\n",
    "\"\"\" Class code for reference:\n",
    "for genre in genres:\n",
    "    movies = [i for i, my_genres in genres_dict.items() if genre in my_genres]   #this part is something i should look at.  looks like it works like a SQL selection, so you're not comparing all movies to all movies\n",
    "    sims = []\n",
    "    for i in movies:\n",
    "        for j in movies:\n",
    "            if i == j: continue\n",
    "            sim = v[i].T.dot(v[j])/(np.linalg.norm(v[i])*np.linalg.norm(v[j]))\n",
    "            sims.append(sim)\n",
    "    print(genre, np.mean(sims))\n",
    "\"\"\" #not too different from my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western \tAverage similarity\t 0.10016602690374397 \n",
      "\n",
      "Comedy \tAverage similarity\t 0.07559470723027109 \n",
      "\n",
      "Children \tAverage similarity\t 0.27229484186459524 \n",
      "\n",
      "Crime \tAverage similarity\t 0.06696085357633019 \n",
      "\n",
      "Musical \tAverage similarity\t 0.27501819151074625 \n",
      "\n",
      "Adventure \tAverage similarity\t 0.07135447165040337 \n",
      "\n",
      "Drama \tAverage similarity\t 0.06768486911697487 \n",
      "\n",
      "Horror \tAverage similarity\t 0.2492613777469407 \n",
      "\n",
      "War \tAverage similarity\t 0.09797277183531476 \n",
      "\n",
      "Documentary \tAverage similarity\t 0.33119984785774526 \n",
      "\n",
      "Romance \tAverage similarity\t 0.08462725574907316 \n",
      "\n",
      "Animation \tAverage similarity\t 0.3071465761929819 \n",
      "\n",
      "Film-Noir \tAverage similarity\t 0.4019363901421902 \n",
      "\n",
      "Sci-Fi \tAverage similarity\t 0.14538503096293928 \n",
      "\n",
      "Mystery \tAverage similarity\t 0.06376430882883291 \n",
      "\n",
      "Fantasy \tAverage similarity\t 0.09753358168777176 \n",
      "\n",
      "IMAX \tAverage similarity\t 0.3276036130440015 \n",
      "\n",
      "Action \tAverage similarity\t 0.11896151409934627 \n",
      "\n",
      "Thriller \tAverage similarity\t 0.0824171365560819 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in similarity_dict:\n",
    "    print(item, \"\\tAverage similarity\\t\", similarity_dict[item][0]/similarity_dict[item][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that film-noirs are the most similar to each other, and mysteries the least similar.\n",
    "The autograder accepted these answers as within tolerance but had slightly different similarity scores (and chose Crime as least similar, not Mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western -0.06542239168968507\n",
      "Comedy nan\n",
      "Children 0.052622799640206684\n",
      "Crime -0.04343356330561578\n",
      "Musical 0.031184848280891242\n",
      "Adventure -0.025442389830672246\n",
      "Drama -0.029766456172157597\n",
      "Horror -0.05380309653999443\n",
      "War -0.07877965598549858\n",
      "Documentary 0.05318609130271492\n",
      "Romance 0.003690483753478649\n",
      "Animation 0.04886051784040335\n",
      "Film-Noir -0.05521928140636941\n",
      "Sci-Fi -0.04034487783480902\n",
      "Mystery -0.042793202368592465\n",
      "Fantasy -0.007804748292963473\n",
      "IMAX -0.04512889018118615\n",
      "Action -0.039017135410493196\n",
      "Thriller -0.037633040928335894\n"
     ]
    }
   ],
   "source": [
    "comedy_dict = {}\n",
    "_, _, v, _ = model\n",
    "for genre in genres:\n",
    "    comedies = [i for i, my_genres in genres_dict.items() if \"Comedy\" in my_genres]\n",
    "for genre in genres:\n",
    "    movies = [i for i, my_genres in genres_dict.items() if genre in my_genres]\n",
    "    sims = []\n",
    "    for i in comedies:\n",
    "        for j in movies:\n",
    "            if i == j: continue\n",
    "            if j in comedies: continue\n",
    "            sim = v[i].T.dot(v[j])/(np.linalg.norm(v[i])*np.linalg.norm(v[j]))\n",
    "            sims.append(sim)\n",
    "    print(genre, np.mean(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children category is most similar, and War category is least similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional HW: Collaborative filtering and the SVD\n",
    "### Eigen Vector and the Half-Blood Python\n",
    "\n",
    "Singular value decompositions: decomposing a large (mostly empty) matrix Y into the product of\n",
    "n x k matrix U and k x m matrix V.T\n",
    "\n",
    "Using black magic to construct orthogonal bases for U and V, we can then write U and V.T as a product of\n",
    "three matrices that are each composed of eigen-stuff.\n",
    "\n",
    "We will derive U and V by having Python calculate the eigen-stuff for a small matrix Y, then compose U and V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[4,3,1],[1,3,-2],[5,2,3]])\n",
    "(r_eval, r_evec) = np.linalg.eig(Y.T @ Y)\t# gives right eigenvalues and eigenvectors\n",
    "(l_eval, l_evec) = np.linalg.eig((Y @ Y.T).T)\t# gives left eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "((r_eval, r_evec),\\\n",
    "(l_eval, l_evec))\n",
    "\n",
    "r_eval = r_eval[0::2] #drop the middle eigenvalue\n",
    "r_evec = r_evec[0::2,:]\n",
    "l_eval = l_eval[0::2]\n",
    "l_evec = l_evec[0::2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the eigenvalues and see that two of them are essentially zero, suggesting that we can take the rank 2 principal component of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([63.97999199, 14.02000801]),\n",
       "  array([[ 0.80844891,  0.57735027,  0.11435482],\n",
       "         [ 0.30519027, -0.57735027,  0.75731471]])),\n",
       " (array([63.97999199, 14.02000801]),\n",
       "  array([[ 0.6311939 ,  0.75180941, -0.19072721],\n",
       "         [ 0.74565815, -0.52048344,  0.41604197]])),\n",
       " (array([[1.],\n",
       "         [1.]]),\n",
       "  array([[1.],\n",
       "         [1.]])))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Big_Lambda = np.array([[r_eval[0]**.5,0],[0,r_eval[1]**0.5]])\n",
    "\n",
    "(   (r_eval, r_evec),\\\n",
    "    (l_eval, l_evec),\\\n",
    "    (r_norms, l_norms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_V = np.array([r_evec[0],r_evec[1]]).T\n",
    "matrix_U = np.array([l_evec[0],l_evec[1]]).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.93375427  1.30294768  2.69176554]\n",
      " [ 4.26686222  4.59709199 -0.78822497]\n",
      " [-0.75792788 -1.78018892  1.00528635]]\n"
     ]
    }
   ],
   "source": [
    "#print(matrix_U,matrix_V.T)\n",
    "print(matrix_U@Big_Lambda@matrix_V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, this matrix bears only minor resemblance to the source matrix Y.  I'm not sure what to make of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
